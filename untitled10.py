# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dYVtAjsNh9Sk4_xhJtoVSpSPBJWVaemo
"""

!pip install python-docx
import re
import numpy as np
from docx import Document
from typing import List
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

BUILTIN_STOPWORDS = set("""
a about above after again against all am an and any are aren't as at be because been before being below
between both but by can can't cannot could couldn't did didn't do does doesn't doing don't down during
each few for from further had hadn't has hasn't have haven't having he he'd he'll he's her here here's
hers herself him himself his how how's i i'd i'll i'm i've if in into is isn't it it's its itself let's
me more most mustn't my myself no nor not of off on once only or other ought our ours ourselves out over
own same shan't she she'd she'll she's should shouldn't so some such than that that's the their theirs
them themselves then there there's these they they'd they'll they're they've this those through to too
under until up very was wasn't we we'd we'll we're we've were weren't what what's when when's where where's
which while who who's whom why why's with won't would wouldn't you you'd you'll you're you've your yours yourself yourselves
""".split())


# Step 1: Load and Clean Text

def load_paragraphs_from_docx(docx_path: str) -> List[str]:
    """Reads DOCX and returns paragraphs longer than 30 characters."""
    document = Document(docx_path)
    return [p.text.strip() for p in document.paragraphs if len(p.text.strip()) > 30]


# Step 2: Preprocess Text

def preprocess_text(text: str) -> str:
    """Lowercase, remove punctuation, remove stopwords."""
    text = text.lower()
    text = re.sub(r"[^a-z\s]", "", text)  # remove punctuation/numbers
    tokens = [word for word in text.split() if word not in BUILTIN_STOPWORDS]
    return " ".join(tokens)

def preprocess_all(paragraphs: List[str]) -> List[str]:
    """Apply preprocessing to a list of paragraphs."""
    return [preprocess_text(p) for p in paragraphs]

# Step 3: Vectorize and Cluster

def vectorize_and_cluster(texts: List[str], num_clusters: int):
    """Creates TF-IDF matrix and runs KMeans clustering."""
    vectorizer = TfidfVectorizer(max_df=0.85, min_df=1, ngram_range=(1, 3))
    tfidf_matrix = vectorizer.fit_transform(texts)

    # Choose sensible number of clusters
    num_clusters = min(num_clusters, len(texts))
    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
    kmeans.fit(tfidf_matrix)
    return kmeans, tfidf_matrix

# Step 4: Extract Topics

def extract_cluster_topics(paragraphs: List[str], labels: np.ndarray, centers: np.ndarray, matrix: np.ndarray, num_topics: int) -> List[str]:
    """Extract top representative topic title per cluster."""
    topics = []
    for cluster_id in range(len(centers)):
        cluster_indices = np.where(labels == cluster_id)[0]
        if len(cluster_indices) == 0:
            continue
        # Pick the paragraph closest to the centroid
        best_idx = cluster_indices[0]  # can improve with cosine similarity
        topic_title = paragraphs[best_idx].split('.')[0].strip().title()
        if topic_title not in topics:
            topics.append(topic_title)
        if len(topics) >= num_topics:
            break
    return topics


# Step 5: Full Model Runner
# ---------------------------
def generate_clean_topics(docx_path: str, num_topics: int) -> List[str]:
    """Run the full topic generation pipeline."""
    # Load and preprocess
    raw_paragraphs = load_paragraphs_from_docx(docx_path)
    processed_texts = preprocess_all(raw_paragraphs)

    # Vectorize and cluster
    kmeans_model, tfidf_matrix = vectorize_and_cluster(processed_texts, num_topics)

    # Extract topic titles
    topics = extract_cluster_topics(
        raw_paragraphs,
        kmeans_model.labels_,
        kmeans_model.cluster_centers_,
        tfidf_matrix,
        num_topics
    )
    return topics


# MAIN EXECUTION

if __name__ == "__main__":
    print("ðŸ“„ Samkhya Topic Extraction Engine")
    docx_path = input("Enter the path to your DOCX file: ").strip()
    num_input = input("How many topics do you want to extract (e.g., 30 or 50)? ").strip()

    try:
        num_topics = int(num_input)
        if num_topics <= 0:
            raise ValueError("Number must be positive.")
        topics = generate_clean_topics(docx_path, num_topics)

        print("\n Extracted Topics:\n")
        for i, topic in enumerate(topics, 1):
            print(f"{i}) {topic}")
    except Exception as e:
        print(f" Error: {e}")